

class HuggingFaceCompatible
    hf_class_type: 


    # this thing can be replaced by the subclass lke the BertModel if there's any params mismatch
    def normalize_hf_key_for_eqx(key: str):

        return key


class BertModel(HuggingFaceCompatible[transformers.BertModel], eqx.Module)

class BertForMaskedLM(HuggingFaceCompatible[transformers.BertForMaskedLM],eqx.Module)


by putting the generics, can we set the class_type, and use that on the HuggingFaceCompatible to use from_pretrained



on the huggingfacecompatible with use the class_type to initalize" the module and get the state_dict,

see _huggingface.py


import types
import typing as tp

import equinox as eqx
import dataclasses as dc
import jax
import jax.tree_util as jtu
from jax import lax, P


class ModuleWithShardingConstraint(eqx.Module):

    """
    Mixin providing __call__ that applies with_sharding_constraint to inputs
    and outputs. By default, shards the last dimension of any Array found in
    args/kwargs or outputs, controlled by shard_in_last_dim/shard_out_last_dim.

    This mixin must be placed before the original class in MRO so that its
    __call__ overrides the base implementation, and then delegates to super().
    """

    _axis_name: str = eqx.field(static=True, default="tp")
    _tp_dim_index: int = eqx.field(static=True, default=-1)  # which dim to shard; -1 means last
    _tp_shard_in: bool = eqx.field(static=True, default=False)
    _tp_shard_out: bool = eqx.field(static=True, default=False)
    _tp_in_spec_fn: tp.Optional[tp.Callable[[jax.Array], tp.Any]] = eqx.field(
        static=True, default=None
    )
    _tp_out_spec_fn: tp.Optional[tp.Callable[[jax.Array], tp.Any]] = eqx.field(
        static=True, default=None
    )

    def _spec_for(self, arr: jax.Array, shard: bool, is_input: bool):
        if not isinstance(arr, jax.Array):
            return None
        if arr.ndim == 0:
            return None
        # If a custom spec function is provided, prefer it
        spec_fn = self._tp_in_spec_fn if is_input else self._tp_out_spec_fn
        if spec_fn is not None:
            return spec_fn(arr)

        axis = self._axis_name if shard else None
        # Choose target dim with support for negatives (like numpy)
        dim = self._tp_dim_index if self._tp_dim_index >= 0 else arr.ndim + self._tp_dim_index
        if dim < 0 or dim >= arr.ndim:
            # Out of range; skip
            return None
        parts = [None] * arr.ndim
        parts[dim] = axis
        return P(*parts)

    def _constrain_tree(self, tree, shard: bool, *, is_input: bool):
        def _apply(x):
            if isinstance(x, jax.Array):
                ps = self._spec_for(x, shard, is_input)
                if ps is not None:
                    return lax.with_sharding_constraint(x, ps)
            return x

        return jtu.tree_map(_apply, tree)

    def __call__(self, *args, **kwargs):  # type: ignore[override]
        # Apply input constraints
        args = self._constrain_tree(args, self._tp_shard_in, is_input=True)
        kwargs = self._constrain_tree(kwargs, self._tp_shard_in, is_input=True)
        # Delegate to original __call__ via MRO
        out = super().__call__(*args, **kwargs)  # type: ignore[misc]
        # Apply output constraints
        out = self._constrain_tree(out, self._tp_shard_out, is_input=False)
        return out


def make_module_with_sharding_constraint(
    module: tp.Any,
    *,
    axis_name: str,
    dim_index: int,
    shard_in: bool,
    shard_out: bool,
    in_spec_fn: tp.Optional[tp.Callable[[jax.Array], tp.Any]] = None,
    out_spec_fn: tp.Optional[tp.Callable[[jax.Array], tp.Any]] = None,
):
    base_cls = module.__class__
    name = f"ModuleWithShardingConstraint{base_cls.__name__}"
    new_cls = type(name, (ModuleWithShardingConstraint, base_cls), {})
    new_cls = dc.dataclass(new_cls)  # type: ignore[misc]

    obj = object.__new__(new_cls)
    for f in dc.fields(base_cls):
        try:
            setattr(obj, f.name, getattr(module, f.name))
        except Exception:
            pass

    setattr(obj, "_tp_axis_name", axis_name)
    setattr(obj, "_tp_dim_index", dim_index)
    setattr(obj, "_tp_shard_in", shard_in)
    setattr(obj, "_tp_shard_out", shard_out)
    setattr(obj, "_tp_in_spec_fn", in_spec_fn)
    setattr(obj, "_tp_out_spec_fn", out_spec_fn)
    return obj


def as_column_parallel(module: tp.Any, axis_name: str, *, dim_index: int = -1):
    """Replicate inputs; shard outputs on the specified dim (column-parallel).

    By default shards the last activation dim (common for vectors coming out of
    per-token projections under vmap).
    """
    return make_module_with_sharding_constraint(
        module,
        axis_name=axis_name,
        dim_index=dim_index,
        shard_in=False,
        shard_out=True,
    )


def as_row_parallel(
    module: tp.Any,
    axis_name: str,
    *,
    dim_index: int = -1,
    out_sharded: bool = True,
):
    """Shard inputs on specified dim (row-parallel).

    If out_sharded is False, outputs are constrained to replicate, which tends
    to induce an all-reduce at the boundary.
    """
    return make_module_with_sharding_constraint(
        module,
        axis_name=axis_name,
        dim_index=dim_index,
        shard_in=True,
        shard_out=out_sharded,
    )


btnh = ((data), )

tensor_parallel(
module,
axis_name
shard
*,
input_spec
output_spec,
mha_spec,
)
