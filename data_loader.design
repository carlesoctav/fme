i want you to create a set of function to make next token prediction datasets.
here's the spec


on the data/next_token_prediction please create

def hf_next_token_prediction_datasets(
    dataset: hf_datasets.Dataset, 
    columns: list[str],
    tokenize: bool,
    tokenizer_path: str, 
    max_target_length: int, 
    axis_name: str
    global_batch_size, 
    *, 
    rekey_after: dict[str, str]:
    shuffle: bool = True, 
    seed: int = 0,
    add_bos: bool = True, 
    add_eos: bool = True, 
    packing: bool = False, 
    num_threads=1, 
    drop_remainder = True,
    worker_count = 1,
    already_shard: False, 
    current_process_id: int = jax.process_index(), 
    total_process = jax.process_count(), 
)-> grain.IterDataset:

just basicly create a grain.iterdataset, for next token prediction pipeline
first making the dataset into grain.mapdataset (randomacesssource)
then shard the datasets into total_process dataset by doin dataset[xx::xx] if not already_shard
then apply sequence of transformation like
tokenize
addTarget/normalize
check  ~/maxtext/input_pipeline/_grain_data_processing.py
check  ~/xlstm-jax/xlstm-jax/dataset/_grain_data_processing.py


then make this as grain.IterDataset

try to packing if packing == True, 
else pad to maxlength
and then maybe reformat lazy_packing some of this newly created thing like from the packing
create a target by shifting the input by one,

on the 

def make_grain_multihost_iterator() -> tp.grainIterator
which basicly add make global_array prehook
this is the same as the make_multihost_iterator

def make_next_token_prediction_iterator(
    grain_datasets: list[grain.IterDataset] | grain.IterDataset,
    dataset_lengths: list[int] | int,
    global_mesh: Mesh,
    global_batch_size: int,
    dataloading_host_count: int,
    dataset_weights: list[float] | None = None,
    worker_count: int = 1,
    worker_buffer_size: int = 1,
    drop_remainder: bool = True,
    batch_class: type = LLMBatch,
    reset_after_epoch: bool = False,
    use_thread_prefetch: bool = False,
    batch_rampup_factors: dict[str, float] | None = None,
) -> MultiHostDataLoadIterator:



then we've one function called make_next_token_prediction


------------------------------------


please read this before implementing my spec

check  ~/maxtext/input_pipeline/_grain_data_processing.py
check  ~/maxtext/input_pipeline
check  ~/xlstm-jax/xlstm-jax/dataset/_grain_data_processing.py
check  ~/xlstm-jax/xlstm-jax/dataset

and also check the spec on grain maybe by search .venv or read the docs

i want you to create a set of function to make next token prediction datasets.
here's the spec

on the data/next_token_prediction.py



1. simple interface, we provide the necesary transformation to create data_loader
dataset = load_dataset("klazs;djfalksdf") # must be dataset, not dictdatasets, iterabledatasets
nstp_operation, batch_class = nex_token_prediction_transforms(
    dataset_type: ["huggingface", "arrayrecord", "tf"]
    column: str,
    max_length,
    tokenizer: huggingfacetokenzier Tokenizer with __call__, encode, decode function 
    is_tokenize,
    add_bos:
    add_eos
    packing: false,
    and many more like
    padding, etc see the xlst-jax version
) -> List[operation (MapTransform) | (DataTransform)], batch_class (namedtuple to store all the item of batch)

if packing == True, it required a iterdataset i think we put this on the DataTransform class,
    we need to check wether or not we apply this on the datasets, or the .map of datasets (or operations on the grain.DataLoader)
    on the prepare_data_loader:

    if isisntance(trans, DataTransforms):
        datasets = trans(data) (probably just to_iter_datasets, and FirstIterFitdatasets ....)
    else:
        datasets.map(trans)
    we need to apply this sequentially, and on the correct orders 


on _training.py

data_loader = prepare_data_loader(
    datasets: [IndexableDS]
    operations,
    global_batch_size, 
    batch_class: type = LLMBatch,
    worker_count: int = 1,
    worker_buffer_size: int = 1,
    drop_remainder: bool = True,
    reset_after_epoch: bool = False,
    use_thread_prefetch: bool = False,
    batch_rampup_factors: dict[str, float] | None = None,
)-> MultiHostData[Iterator[batch_class]] | SingleHostData[Iterator[Batch_class]] (either by grain.DataLoader)
     for batch_rampup_factors, idk, maybe you need to convert this datasets into that thing first
     see xlstm
    we use DataLoader(ds, with sampler and correct shard_options) if we dont required to mv the datasets into iter_datasets
    else we use .method to do transformations, even we need to manually shard this things
    after that, wrap this so i can add hook make_global_array when iter



couple things

i think we can make
make_next_token_prediction and make_next_token_prediction_iterator as 
make_dataloader(), and maybe_make_multihost_iterator(), i want tthis two thing as the main entrace for user
to apply all the predfiend operations from the next_token_prediction_transforms()
and this thing should be on _training.py, and this two function is the general function not spesific to certain pretrained method

other than that remove the ArrayLike and just use jaxtyping Array
thanks


also try to create masked_language_modeling.py 
