HloModule jit_init, entry_computation_layout={()->(f32[16,8]{1,0}, f32[16]{0}, f32[32,16]{1,0}, f32[32]{0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}, num_partitions=8

ENTRY %main.11 () -> (f32[16,8], f32[16], f32[32,16], f32[32]) {
  %constant.0 = f32[16,8]{1,0} constant({...}), sharding={replicated}
  %sharding_constraint.2 = f32[16,8]{1,0} copy(%constant.0), sharding={replicated}, metadata={op_name="jit(init)/sharding_constraint" source_file="/home/carlesoctav/personal/modernizing-indo-nlp/temp/test_my_great_tp_plan.py" source_line=51 source_end_line=51 source_column=26 source_end_column=26}
  %constant.3 = f32[] constant(0)
  %broadcast.4 = f32[16]{0} broadcast(%constant.3), dimensions={}, sharding={replicated}, metadata={op_name="broadcast.3"}
  %sharding_constraint.5 = f32[16]{0} copy(%broadcast.4), sharding={replicated}, metadata={op_name="jit(init)/sharding_constraint" source_file="/home/carlesoctav/personal/modernizing-indo-nlp/temp/test_my_great_tp_plan.py" source_line=51 source_end_line=51 source_column=26 source_end_column=26}
  %constant.6 = f32[32,16]{1,0} constant({...}), sharding={replicated}
  %sharding_constraint.0 = f32[32,16]{1,0} copy(%constant.6), sharding={replicated}, metadata={op_name="jit(init)/sharding_constraint" source_file="/home/carlesoctav/personal/modernizing-indo-nlp/temp/test_my_great_tp_plan.py" source_line=51 source_end_line=51 source_column=26 source_end_column=26}
  %broadcast.8 = f32[32]{0} broadcast(%constant.3), dimensions={}, sharding={replicated}, metadata={op_name="broadcast.6"}
  %sharding_constraint.1 = f32[32]{0} copy(%broadcast.8), sharding={replicated}, metadata={op_name="jit(init)/sharding_constraint" source_file="/home/carlesoctav/personal/modernizing-indo-nlp/temp/test_my_great_tp_plan.py" source_line=51 source_end_line=51 source_column=26 source_end_column=26}
  ROOT %tuple.10 = (f32[16,8]{1,0}, f32[16]{0}, f32[32,16]{1,0}, f32[32]{0}) tuple(%sharding_constraint.2, %sharding_constraint.5, %sharding_constraint.0, %sharding_constraint.1)
}

