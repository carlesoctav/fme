see _trainer_module.py

there's 3 thing thas has not been yet implemented here,

the metrics handling
the logging handling
and the model_checkpoint handling

but before that i want to change my api design and just make it similar to the lightning fabric (instead of lightning module)


the first thing that i want is something like this

trainer_util = TrainerUtil(
    ParallelConfig,
    mesh
    parallelism_plan:
)
basicly just init the class,


later i can use 

module (or modules), optimizer (or optimizers) = trainer_util.setup_module_opts(module, grad_txs, wrts, paralleism_plans)

both args take M | sequence of M and return M | seuqence of that args
pleas use tp.overload to make typing differnt

this setup module basicly craete the optimizer, and put the module and opt state based on the parallelism plan,
after that we use 


also i want make ti possible to take abstract_module instead of module, first we need to figure it out how to reinit the weight 


then i want trainer_util.make_train_step(loss_function, train_step)
previously we log if it's single_module, if yes we can input loss_function, but if it's not single_module, user need to provide the train_step


then we can use the train_step()


maybe_do(function, function_args, function_kwargs, curr_step, required_step)


for i, batch in data_loader:
    model, optimizer, aux = train_step(model, optimizer, batch, key = key)
    global_metrics = compute_metrics(global_metrics, aux)
    maybe_write(global_metrics, i)
    maybe_checkpoint(checkpoint_manager, i, ...)

then u need to create this functionally to compute the metrics, logs the metrics and also checkpoiiting

see ~/personal/maxtext for the checkpoint implemnetation
for the logger see ~/personal/xlstm-jax
i alreadyu create a pytree called Metric, and maybe we can use this :d



i hate our reinit functionality,
it should be something that we implement on the nn.Linear, and other atomic module


let's called init_weights

init_weights(self) -> Module
    return the same module but jax.array



and on The more complex module like

we can provide init_weights_plan() on 
BertModel

init_weights_plan():
    pass


def init_weights_plan(self, module):
        """Initialize the weights"""
        if isinstance(module, nn.Linear):
            # Slightly different from the TF version which uses truncated_normal for initialization
            # cf https://github.com/pytorch/pytorch/pull/5617
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
        elif isinstance(module, RobertaLMPredictionHead):
            module.bias.data.zero_()


and use it something like  this

init_module(module, init_weights_plan : OPtional)-> module:
    if not init_weight_plan:
        class_plan = getattr(module.__class__, init_weights_plan)
        after that we use apply_transoforms to create new module :D 
        if not class_plan:

            def _f():
                
                pass
            iter for atomic module and use the deafult init_weights()
            jtu.tree_map()



also on the make_train_step, dont maek loss as the default aux, just let user to decide which aux they want to keep
