see _trainer_module.py

there's 3 thing thas has not been yet implemented here,

the metrics handling
the logging handling
and the model_checkpoint handling

but before that i want to change my api design and just make it similar to the lightning fabric (instead of lightning module)


the first thing that i want is something like this

trainer_util = TrainerUtil(
    ParallelConfig,
    mesh
    parallelism_plan:
)
basicly just init the class,


later i can use 

module (or modules), optimizer (or optimizers) = trainer_util.setup_module_opts(module, grad_txs, wrts, paralleism_plans)

both args take M | sequence of M and return M | seuqence of that args
pleas use tp.overload to make typing differnt

this setup module basicly craete the optimizer, and put the module and opt state based on the parallelism plan,
after that we use 


also i want make ti possible to take abstract_module instead of module, first we need to figure it out how to reinit the weight 


then i want trainer_util.make_train_step(loss_function, train_step)
previously we log if it's single_module, if yes we can input loss_function, but if it's not single_module, user need to provide the train_step


then we can use the train_step()


maybe_do(function, function_args, function_kwargs, curr_step, required_step)


for i, batch in data_loader:
    model, optimizer, aux = train_step(model, optimizer, batch, key = key)
    global_metrics = compute_metrics(global_metrics, aux)
    maybe_write(global_metrics, i)
    maybe_checkpoint(checkpoint_manager, i, ...)

then u need to create this functionally to compute the metrics, logs the metrics and also checkpoiiting

see ~/personal/maxtext for the checkpoint implemnetation
for the logger see ~/personal/xlstm-jax
i alreadyu create a pytree called Metric, and maybe we can use this :d
